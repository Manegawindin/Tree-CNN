{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Input, Flatten, MaxPooling2D\n",
    "from tensorflow.keras.models import Model\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining class to support Tree-CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN (object):\n",
    "    def __init__(self, num_classes, input_shape):\n",
    "#         input_layer, l = self.create_network_base(num_classes, input_shape)\n",
    "        self.num_classes = num_classes\n",
    "        self.input_shape = input_shape\n",
    "        self.model = tf.keras.applications.resnet.ResNet50(include_top=True, weights=None, classes=num_classes, input_shape=input_shape)\n",
    "#         Model(input_layer, l)\n",
    "        self.model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "#     def create_network_base(self, num_classes, input_shape):\n",
    "#         input_layer = Input(shape=input_shape)\n",
    "#         l = Conv2D(64, 3, activation='relu')(input_layer)\n",
    "#         l = MaxPooling2D()(l)\n",
    "#         l = Conv2D(32, 3, activation='relu')(l)\n",
    "#         l = MaxPooling2D()(l)\n",
    "#         l = Conv2D(32, 3,activation='relu')(l)\n",
    "#         l = MaxPooling2D()(l)\n",
    "#         l = Conv2D(16, 3, activation='relu')(l)\n",
    "#         l = MaxPooling2D()(l)\n",
    "#         l = Flatten()(l)\n",
    "#         l = Dense(64, activation='relu')(l)\n",
    "#         l = Dense(num_classes, activation='softmax')(l)\n",
    "        \n",
    "#         return input_layer, l\n",
    "    \n",
    "    def train(self, X, Y):\n",
    "        cb = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15)\n",
    "        print(X.shape, Y.shape)\n",
    "        self.model.fit(X, Y, batch_size=16, epochs=100000, callbacks=[cb], validation_split=0.1)\n",
    "\n",
    "    def remove_class(self, idx_to_remove):\n",
    "#         input_layer, l = self.create_network_base((self.num_classes-1), self.input_shape)\n",
    "        new_model = tf.keras.applications.resnet.ResNet50(include_top=True, weights=None, classes=(self.num_classes-1), input_shape=self.input_shape)\n",
    "#     Model(input_layer, l)\n",
    "        \n",
    "        for idx in range(len(self.model.layers)-1):\n",
    "            if len(self.model.layers[idx].get_weights()) == 0 :\n",
    "                continue\n",
    "            if len(self.model.layers[idx].get_weights()) == 2 :\n",
    "                wi = self.model.layers[idx].get_weights()[0]\n",
    "                bi = self.model.layers[idx].get_weights()[1]\n",
    "                new_model.layers[idx].set_weights((wi, bi))\n",
    "            if len(self.model.layers[idx].get_weights()) == 4 :\n",
    "                wi = self.model.layers[idx].get_weights()[0]\n",
    "                bi = self.model.layers[idx].get_weights()[1]\n",
    "                wi2 = self.model.layers[idx].get_weights()[2]\n",
    "                bi2 = self.model.layers[idx].get_weights()[3]\n",
    "                new_model.layers[idx].set_weights((wi, bi, wi2, bi2))\n",
    "        \n",
    "        # Copy a already treined part of last layer\n",
    "        old_w = self.model.layers[-1].get_weights()[0]\n",
    "        new_w = new_model.layers[-1].get_weights()[0]\n",
    "        old_bias = self.model.layers[-1].get_weights()[1]\n",
    "        new_bias = new_model.layers[-1].get_weights()[1]\n",
    "\n",
    "        for i in range(old_w.shape[0]):\n",
    "            aux = 0\n",
    "            for j in range(old_w.shape[1]):\n",
    "                if j != idx_to_remove:\n",
    "                    new_w[i][aux] = old_w[i][j]\n",
    "                    aux = aux + 1\n",
    "        aux = 0\n",
    "        for i in range(old_bias.shape[0]):\n",
    "            if i != idx_to_remove:\n",
    "                new_bias[aux] = old_bias[i]\n",
    "                aux = aux + 1\n",
    "\n",
    "        new_model.layers[-1].set_weights((new_w, new_bias))\n",
    "        \n",
    "        self.model = new_model\n",
    "        self.model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        self.num_classes = self.num_classes - 1\n",
    "        \n",
    "    def add_class(self):\n",
    "#         input_layer, l = self.create_network_base((self.num_classes+1), self.input_shape)\n",
    "        new_model = tf.keras.applications.resnet.ResNet50(include_top=True, weights=None, classes=(self.num_classes+1), input_shape=self.input_shape)\n",
    "#     Model(input_layer, l)\n",
    "        \n",
    "        for idx in range(len(self.model.layers)-1):\n",
    "            if len(self.model.layers[idx].get_weights()) == 0 :\n",
    "                continue\n",
    "            if len(self.model.layers[idx].get_weights()) == 2 :\n",
    "                wi = self.model.layers[idx].get_weights()[0]\n",
    "                bi = self.model.layers[idx].get_weights()[1]\n",
    "                new_model.layers[idx].set_weights((wi, bi))\n",
    "            if len(self.model.layers[idx].get_weights()) == 4 :\n",
    "                wi = self.model.layers[idx].get_weights()[0]\n",
    "                bi = self.model.layers[idx].get_weights()[1]\n",
    "                wi2 = self.model.layers[idx].get_weights()[2]\n",
    "                bi2 = self.model.layers[idx].get_weights()[3]\n",
    "                new_model.layers[idx].set_weights((wi, bi, wi2, bi2))\n",
    "        \n",
    "        # Copy a already treined part of last layer\n",
    "        old_w = self.model.layers[-1].get_weights()[0]\n",
    "        new_w = new_model.layers[-1].get_weights()[0]\n",
    "        old_bias = self.model.layers[-1].get_weights()[1]\n",
    "        new_bias = new_model.layers[-1].get_weights()[1]\n",
    "\n",
    "        for i in range(old_w.shape[0]):\n",
    "            for j in range(old_w.shape[1]):\n",
    "                new_w[i][j] = old_w[i][j]\n",
    "        for i in range(old_bias.shape[0]):\n",
    "            new_bias[i] = old_bias[i]\n",
    "\n",
    "        new_model.layers[-1].set_weights((new_w, new_bias))\n",
    "        \n",
    "        self.model = new_model\n",
    "        self.model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "        self.num_classes = self.num_classes + 1\n",
    "        \n",
    "    def pred(self, img):\n",
    "        return self.model.predict(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NodeList (object):\n",
    "    def __init__(self, label, values, nodes, class_position):\n",
    "        self.label = label\n",
    "        self.values = values\n",
    "        self.nodes = nodes\n",
    "        self.class_position = class_position\n",
    "    def __str__(self):\n",
    "        return {'label': self.label, 'values': self.values, 'nodes':self.nodes, 'class_position': self.class_position}\n",
    "    def __unicode__(self):\n",
    "        return {'label': self.label, 'values': self.values, 'nodes':self.nodes, 'class_position': self.class_position}\n",
    "    def __repr__(self):\n",
    "        return 'label: ' + str(self.label) + ' values: ' + str(self.values) + ' nodes: ' + str(self.nodes)+' class_pos: '+ str(self.class_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CnnNode (object):\n",
    "    def __init__(self, num_classes, labels = [], input_shape=(28,28,1), max_leafes=10):\n",
    "        self.net = CNN(num_classes, input_shape)\n",
    "        self.num_classes = num_classes\n",
    "        self.childrens = [label for label in labels]\n",
    "        self.childrens_leaf = [True for _ in range(num_classes)]\n",
    "        self.labels = labels\n",
    "        self.max_leafes = max_leafes\n",
    "        self.labels_transform = {}\n",
    "        for nc in range(num_classes):\n",
    "            self.labels_transform[labels[nc]] = []\n",
    "            self.labels_transform[labels[nc]].append(labels[nc])\n",
    "        \n",
    "    \n",
    "    def get_num_leafnodes(self):\n",
    "        count = 0\n",
    "        for is_leaf in self.childrens_leaf:\n",
    "            if is_leaf:\n",
    "                count = count + 1\n",
    "        return count\n",
    "    \n",
    "    def remove_leaf(self, label):\n",
    "        childrens = []\n",
    "        childrens_leaf = []\n",
    "        labels = []\n",
    "        del self.labels_transform[label]\n",
    "        self.num_classes = (self.num_classes - 1)\n",
    "        position_in_net = -1\n",
    "        \n",
    "        for i in range(len(self.labels)):\n",
    "            if self.labels[i] != label:\n",
    "                childrens.append(self.childrens[i])\n",
    "                childrens_leaf.append(self.childrens_leaf[i])\n",
    "                labels.append(self.labels[i])\n",
    "            else:\n",
    "                position_in_net = i\n",
    "                \n",
    "        self.childrens = childrens\n",
    "        self.childrens_leaf = childrens_leaf\n",
    "        self.labels = labels\n",
    "        self.net.remove_class(position_in_net)\n",
    "        \n",
    "    def add_leaf(self, label):\n",
    "        self.childrens.append(label)\n",
    "        self.childrens_leaf.append(True)\n",
    "        self.num_classes = (self.num_classes + 1)\n",
    "        self.labels.append(label)\n",
    "        self.labels_transform[label] = []\n",
    "        self.labels_transform[label].append(label)\n",
    "        self.net.add_class()\n",
    "    \n",
    "    def predict(self, imgs):\n",
    "        vector_output = self.net.pred(imgs)\n",
    "        return vector_output\n",
    "    \n",
    "    def inference(self, imgs):\n",
    "        vector_output = self.predict(imgs)\n",
    "        out = np.array([idx for idx in np.argmax(vector_output, axis=1)])\n",
    "        output = np.array([-1 for _ in range(imgs.shape[0])])\n",
    "\n",
    "        for i, o in zip(range(len(out)), out):\n",
    "            if self.childrens_leaf[o]:\n",
    "                output[i] = self.labels[o]            \n",
    "\n",
    "        # Send images to branches nodes\n",
    "        for child_id in range(len(self.childrens_leaf)):\n",
    "            if not self.childrens_leaf[child_id]:\n",
    "                output[out==child_id] = self.childrens[child_id].inference(imgs[out==child_id])\n",
    "\n",
    "\n",
    "        return output\n",
    "    \n",
    "    def train(self, X, Y):\n",
    "        Y_to_this_nivel = Y.copy()\n",
    "        print(1, self.labels_transform)\n",
    "        for idx in range(len(self.labels)):\n",
    "            for label in self.labels_transform[self.labels[idx]]:\n",
    "                Y_to_this_nivel[Y==label] = idx\n",
    "\n",
    "        Y_true = pd.get_dummies(pd.Series(Y_to_this_nivel)).values\n",
    "        self.net.train(X, Y_true)\n",
    "        \n",
    "        # Send images to branches nodes\n",
    "        for child_id in range(len(self.childrens_leaf)):\n",
    "            if not self.childrens_leaf[child_id]:\n",
    "                self.childrens[child_id].train(X[Y_to_this_nivel==child_id], Y[Y_to_this_nivel==child_id])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeCNN (object):\n",
    "    def __init__(self, initial_labels, alpha=0.1, beta=0.1, input_shape=(28,28,1), max_leafnodes=1000):\n",
    "        num_class_initial = len(initial_labels)\n",
    "        self.root = CnnNode(num_class_initial, labels=initial_labels, input_shape=input_shape)\n",
    "        self.alpha = alpha\n",
    "        self.beta = beta\n",
    "        self.max_leafnodes = max_leafnodes\n",
    "        \n",
    "    \n",
    "    def addTasks(self, imgs_of_classes=[], labels=[]):\n",
    "        self.growTreeCNN(self.root, imgs_of_classes, labels)\n",
    "        \n",
    "    def train(self, X, Y):\n",
    "        self.root.train(X, Y)\n",
    "        \n",
    "    def inference(self, X):\n",
    "        return self.root.inference(X)\n",
    "        \n",
    "        \n",
    "    def growTreeCNN(self, operation_node, imgs_of_classes=[], labels=[]):\n",
    "        def get_Oavg_matrix(node, imgs_of_classes_, labels_):\n",
    "            Oavg = np.zeros(shape=(node.num_classes, 0))\n",
    "            for imgs, label in zip(imgs_of_classes_, labels_):\n",
    "                net_out = node.predict(imgs)\n",
    "                Oavg_i = np.average(net_out, axis=0)\n",
    "                Oavg = np.concatenate(( Oavg, Oavg_i.reshape((Oavg_i.shape[0], 1)) ), axis=1)\n",
    "            return Oavg\n",
    "        \n",
    "        def get_loglikelihood_matrix(Oavg):\n",
    "            return (np.power(np.e, Oavg) / np.sum(np.power(np.e, Oavg), axis=0))\n",
    "        \n",
    "        def generate_listS(llh, labels_in):\n",
    "            listS = []\n",
    "            for i in range(llh.shape[1]):\n",
    "                label = labels_in[i]\n",
    "                values = []\n",
    "                nodes = []\n",
    "\n",
    "                col = llh[:,i].copy()\n",
    "                for _ in range(3):\n",
    "                    max_idx = np.argmax(col)\n",
    "                    values.append(col[max_idx])\n",
    "                    nodes.append(max_idx)\n",
    "                    col[max_idx] = -100\n",
    "\n",
    "                listS.append(NodeList(label, values, nodes, i))\n",
    "\n",
    "            # Sort List S by value of S[i].values[0]\n",
    "            listS.sort(key=lambda node_list: node_list.values[0])\n",
    "            \n",
    "            return listS\n",
    "\n",
    "        llh = get_loglikelihood_matrix(get_Oavg_matrix(operation_node, imgs_of_classes, labels))\n",
    "        \n",
    "        listS = generate_listS(llh, labels)\n",
    "        new_labels = labels\n",
    "        \n",
    "        branches_dest = {}\n",
    "        while len(listS) > 0:\n",
    "            nodeList = listS[0]\n",
    "            rows_to_remove_in_llh = []\n",
    "            if nodeList.values[0] - nodeList.values[1] > self.alpha:\n",
    "                if operation_node.childrens_leaf[nodeList.nodes[0]]:\n",
    "                    operation_node.childrens_leaf[nodeList.nodes[0]] = False\n",
    "                    old_label = operation_node.labels[nodeList.nodes[0]]\n",
    "                    new_label = nodeList.label\n",
    "                    branch_node = CnnNode(2, labels=[old_label, new_label])\n",
    "                    operation_node.childrens[nodeList.nodes[0]] = branch_node\n",
    "                else:\n",
    "                    if nodeList.nodes[0] not in branches_dest:\n",
    "                        branches_dest[nodeList.nodes[0]] = []\n",
    "                    branches_dest[nodeList.nodes[0]].append(nodeList.label)\n",
    "                \n",
    "                operation_node.labels_transform[nodeList.nodes[0]].append(nodeList.label)\n",
    "            elif nodeList.values[1] - nodeList.values[2] > self.beta:\n",
    "                left_is_leafnode = operation_node.childrens_leaf[nodeList.nodes[0]]\n",
    "                right_is_leafnode = operation_node.childrens_leaf[nodeList.nodes[1]]\n",
    "                has_space_in_left = left_is_leafnode or (operation_node.childrens[nodeList.nodes[0]].get_num_leafnodes() < (self.max_leafnodes - 1))\n",
    "                \n",
    "                if right_is_leafnode and has_space_in_left: # if Merge\n",
    "                    if operation_node.childrens_leaf[nodeList.nodes[0]]: # if left is a leaf\n",
    "                        operation_node.childrens_leaf[nodeList.nodes[0]] = False\n",
    "                        old_label = operation_node.labels[nodeList.nodes[0]]\n",
    "                        new_label = operation_node.labels[nodeList.nodes[1]]\n",
    "                        branch_node = CnnNode(2, labels=[old_label, new_label])\n",
    "                        operation_node.childrens[nodeList.nodes[0]] = branch_node\n",
    "                    else:\n",
    "                        operation_node.childrens[nodeList.nodes[0]].add_leaf(operation_node.labels[nodeList.nodes[1]])\n",
    "                        if nodeList.nodes[0] not in branches_dest:\n",
    "                            branches_dest[nodeList.nodes[0]] = []\n",
    "                        branches_dest[nodeList.nodes[0]].append(nodeList.label)\n",
    "                    \n",
    "                    operation_node.labels_transform[nodeList.nodes[0]].append(operation_node.labels[nodeList.nodes[1]])\n",
    "                    operation_node.remove_leaf(operation_node.labels[nodeList.nodes[1]])\n",
    "                    rows_to_remove_in_llh.append(nodeList.nodes[1])\n",
    "                else:\n",
    "                    if left_is_leafnode:\n",
    "                        operation_node.childrens_leaf[nodeList.nodes[0]] = False\n",
    "                        old_label = operation_node.labels[nodeList.nodes[0]]\n",
    "                        new_label = nodeList.label\n",
    "                        branch_node = CnnNode(2, labels=[old_label, new_label])\n",
    "                        operation_node.childrens[nodeList.nodes[0]] = branch_node\n",
    "                    elif right_is_leafnode:\n",
    "                        operation_node.childrens_leaf[nodeList.nodes[1]] = False\n",
    "                        old_label = operation_node.labels[nodeList.nodes[1]]\n",
    "                        new_label = nodeList.label\n",
    "                        branch_node = CnnNode(2, labels=[old_label, new_label])\n",
    "                        operation_node.childrens[nodeList.nodes[1]] = branch_node\n",
    "                    else:\n",
    "                        if operation_node.childrens[nodeList.nodes[0]].get_num_leafnodes() < operation_node.childrens[nodeList.nodes[1]].get_num_leafnodes():\n",
    "                            if nodeList.nodes[0] not in branches_dest:\n",
    "                                branches_dest[nodeList.nodes[0]] = []\n",
    "                            branches_dest[nodeList.nodes[0]].append(nodeList.label)\n",
    "                        else:\n",
    "                            if nodeList.nodes[1] not in branches_dest:\n",
    "                                branches_dest[nodeList.nodes[1]] = []\n",
    "                                print(nodeList)\n",
    "                            branches_dest[nodeList.nodes[1]].append(nodeList.label)\n",
    "                    \n",
    "            else:\n",
    "                operation_node.add_leaf(nodeList.label)\n",
    "                \n",
    "            \n",
    "            # Clean likelihood matrix and labels to recreate listS to next iteration\n",
    "            # Delete column already inserted\n",
    "            llh = np.delete(llh, nodeList.class_position, axis=1)\n",
    "            # Delete rows that was merged\n",
    "            for r in rows_to_remove_in_llh:\n",
    "                llh = np.delete(llh, r, axis=0)\n",
    "            # Delete rows that represent full nodes (just branch/child nodes)\n",
    "            deleted = 0\n",
    "            for i in range(len(operation_node.childrens_leaf)):\n",
    "                if not operation_node.childrens_leaf[i] and (operation_node.childrens[i].get_num_leafnodes() >= self.max_leafnodes):\n",
    "                    llh = np.delete(llh, (i - deleted), axis=0)\n",
    "                    deleted = deleted + 1\n",
    "            # Update labels\n",
    "            del new_labels[nodeList.class_position]\n",
    "            listS = generate_listS(llh, new_labels)\n",
    "        \n",
    "        # Send to sub-nivels\n",
    "        for k, v in branches_dest.items():\n",
    "            imgs_to_send = []\n",
    "            labels_to_send = []\n",
    "            for idx, label in zip(range(len(labels)), labels):\n",
    "                if label in v:\n",
    "                    imgs_to_send.append(imgs_of_classes[idx])\n",
    "                    labels_to_send.append(labels[idx])\n",
    "\n",
    "            self.growTreeCNN(operation_node.childrens[k], imgs_to_send, labels_to_send)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree = TreeCNN([0,1,2,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_initial_classes = 4\n",
    "old_classes_X = x_train[y_train<num_initial_classes].astype(float)\n",
    "old_classes_X = old_classes_X.reshape((old_classes_X.shape[0], old_classes_X.shape[1], old_classes_X.shape[2], 1))\n",
    "old_classes_Y = y_train[y_train<num_initial_classes]\n",
    "\n",
    "new_classes_X = [x_train[y_train==i].reshape((x_train[y_train==i].shape[0], x_train[y_train==i].shape[1], x_train[y_train==i].shape[2], 1 )).astype(float) for i in range(4, 7)]\n",
    "new_classes_Y = [i for i in range(4, 7)]\n",
    "\n",
    "\n",
    "# pd.get_dummies(pd.Series(old_classes_Y))\n",
    "merged_classes_X = x_train[y_train<7].astype(float)\n",
    "merged_classes_X = merged_classes_X.reshape((merged_classes_X.shape[0], merged_classes_X.shape[1], merged_classes_X.shape[2], 1))\n",
    "merged_classes_Y = y_train[y_train<7]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train with firsts classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(new_classes_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.train(old_classes_X, old_classes_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = tree.inference(old_classes_X)\n",
    "Y_true = y_train[y_train<num_initial_classes]\n",
    "print(Y_hat)\n",
    "np.sum(Y_true==Y_hat)/len(Y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Growing network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.addTasks(new_classes_X, [i for i in range(4, 7)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tree.root.net.model.layers[-1].output_shape)\n",
    "print(tree.root.childrens_leaf)\n",
    "print(tree.root.childrens)\n",
    "tree.root.labels_transform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate in old classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = tree.inference(old_classes_X)\n",
    "Y_true = y_train[y_train<num_initial_classes]\n",
    "print(Y_hat)\n",
    "np.sum(Y_true==Y_hat)/len(Y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate in all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = tree.inference(merged_classes_X)\n",
    "Y_true = y_train[y_train<7]\n",
    "print(Y_hat)\n",
    "np.sum(Y_true==Y_hat)/len(Y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retraing with all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.train(merged_classes_X, merged_classes_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-evaluate in old classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = tree.inference(old_classes_X)\n",
    "Y_true = y_train[y_train<num_initial_classes]\n",
    "print(Y_hat)\n",
    "np.sum(Y_true==Y_hat)/len(Y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Re-evaluate in all classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_hat = tree.inference(merged_classes_X)\n",
    "Y_true = y_train[y_train<7]\n",
    "print(Y_hat)\n",
    "np.sum(Y_true==Y_hat)/len(Y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test with CUBS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"resnet50\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_7 (InputLayer)            [(None, 500, 335, 3) 0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1_pad (ZeroPadding2D)       (None, 506, 341, 3)  0           input_7[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv1_conv (Conv2D)             (None, 250, 168, 64) 9472        conv1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv1_bn (BatchNormalization)   (None, 250, 168, 64) 256         conv1_conv[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv1_relu (Activation)         (None, 250, 168, 64) 0           conv1_bn[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pad (ZeroPadding2D)       (None, 252, 170, 64) 0           conv1_relu[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "pool1_pool (MaxPooling2D)       (None, 125, 84, 64)  0           pool1_pad[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_conv (Conv2D)    (None, 125, 84, 64)  4160        pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_bn (BatchNormali (None, 125, 84, 64)  256         conv2_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_1_relu (Activation (None, 125, 84, 64)  0           conv2_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_conv (Conv2D)    (None, 125, 84, 64)  36928       conv2_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_bn (BatchNormali (None, 125, 84, 64)  256         conv2_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_2_relu (Activation (None, 125, 84, 64)  0           conv2_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_conv (Conv2D)    (None, 125, 84, 256) 16640       pool1_pool[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_conv (Conv2D)    (None, 125, 84, 256) 16640       conv2_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_0_bn (BatchNormali (None, 125, 84, 256) 1024        conv2_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_3_bn (BatchNormali (None, 125, 84, 256) 1024        conv2_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_add (Add)          (None, 125, 84, 256) 0           conv2_block1_0_bn[0][0]          \n",
      "                                                                 conv2_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block1_out (Activation)   (None, 125, 84, 256) 0           conv2_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_conv (Conv2D)    (None, 125, 84, 64)  16448       conv2_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_bn (BatchNormali (None, 125, 84, 64)  256         conv2_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_1_relu (Activation (None, 125, 84, 64)  0           conv2_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_conv (Conv2D)    (None, 125, 84, 64)  36928       conv2_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_bn (BatchNormali (None, 125, 84, 64)  256         conv2_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_2_relu (Activation (None, 125, 84, 64)  0           conv2_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_conv (Conv2D)    (None, 125, 84, 256) 16640       conv2_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_3_bn (BatchNormali (None, 125, 84, 256) 1024        conv2_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_add (Add)          (None, 125, 84, 256) 0           conv2_block1_out[0][0]           \n",
      "                                                                 conv2_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block2_out (Activation)   (None, 125, 84, 256) 0           conv2_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_conv (Conv2D)    (None, 125, 84, 64)  16448       conv2_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_bn (BatchNormali (None, 125, 84, 64)  256         conv2_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_1_relu (Activation (None, 125, 84, 64)  0           conv2_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_conv (Conv2D)    (None, 125, 84, 64)  36928       conv2_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_bn (BatchNormali (None, 125, 84, 64)  256         conv2_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_2_relu (Activation (None, 125, 84, 64)  0           conv2_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_conv (Conv2D)    (None, 125, 84, 256) 16640       conv2_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_3_bn (BatchNormali (None, 125, 84, 256) 1024        conv2_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_add (Add)          (None, 125, 84, 256) 0           conv2_block2_out[0][0]           \n",
      "                                                                 conv2_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv2_block3_out (Activation)   (None, 125, 84, 256) 0           conv2_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_conv (Conv2D)    (None, 63, 42, 128)  32896       conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_bn (BatchNormali (None, 63, 42, 128)  512         conv3_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_1_relu (Activation (None, 63, 42, 128)  0           conv3_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_conv (Conv2D)    (None, 63, 42, 128)  147584      conv3_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_bn (BatchNormali (None, 63, 42, 128)  512         conv3_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_2_relu (Activation (None, 63, 42, 128)  0           conv3_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_conv (Conv2D)    (None, 63, 42, 512)  131584      conv2_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_conv (Conv2D)    (None, 63, 42, 512)  66048       conv3_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_0_bn (BatchNormali (None, 63, 42, 512)  2048        conv3_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_3_bn (BatchNormali (None, 63, 42, 512)  2048        conv3_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_add (Add)          (None, 63, 42, 512)  0           conv3_block1_0_bn[0][0]          \n",
      "                                                                 conv3_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block1_out (Activation)   (None, 63, 42, 512)  0           conv3_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_conv (Conv2D)    (None, 63, 42, 128)  65664       conv3_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_bn (BatchNormali (None, 63, 42, 128)  512         conv3_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_1_relu (Activation (None, 63, 42, 128)  0           conv3_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_conv (Conv2D)    (None, 63, 42, 128)  147584      conv3_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_bn (BatchNormali (None, 63, 42, 128)  512         conv3_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_2_relu (Activation (None, 63, 42, 128)  0           conv3_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_conv (Conv2D)    (None, 63, 42, 512)  66048       conv3_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_3_bn (BatchNormali (None, 63, 42, 512)  2048        conv3_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_add (Add)          (None, 63, 42, 512)  0           conv3_block1_out[0][0]           \n",
      "                                                                 conv3_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block2_out (Activation)   (None, 63, 42, 512)  0           conv3_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_conv (Conv2D)    (None, 63, 42, 128)  65664       conv3_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_bn (BatchNormali (None, 63, 42, 128)  512         conv3_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_1_relu (Activation (None, 63, 42, 128)  0           conv3_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_conv (Conv2D)    (None, 63, 42, 128)  147584      conv3_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_bn (BatchNormali (None, 63, 42, 128)  512         conv3_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_2_relu (Activation (None, 63, 42, 128)  0           conv3_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_conv (Conv2D)    (None, 63, 42, 512)  66048       conv3_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_3_bn (BatchNormali (None, 63, 42, 512)  2048        conv3_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_add (Add)          (None, 63, 42, 512)  0           conv3_block2_out[0][0]           \n",
      "                                                                 conv3_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block3_out (Activation)   (None, 63, 42, 512)  0           conv3_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_conv (Conv2D)    (None, 63, 42, 128)  65664       conv3_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_bn (BatchNormali (None, 63, 42, 128)  512         conv3_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_1_relu (Activation (None, 63, 42, 128)  0           conv3_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_conv (Conv2D)    (None, 63, 42, 128)  147584      conv3_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_bn (BatchNormali (None, 63, 42, 128)  512         conv3_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_2_relu (Activation (None, 63, 42, 128)  0           conv3_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_conv (Conv2D)    (None, 63, 42, 512)  66048       conv3_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_3_bn (BatchNormali (None, 63, 42, 512)  2048        conv3_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_add (Add)          (None, 63, 42, 512)  0           conv3_block3_out[0][0]           \n",
      "                                                                 conv3_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv3_block4_out (Activation)   (None, 63, 42, 512)  0           conv3_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_conv (Conv2D)    (None, 32, 21, 256)  131328      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_bn (BatchNormali (None, 32, 21, 256)  1024        conv4_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_1_relu (Activation (None, 32, 21, 256)  0           conv4_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_conv (Conv2D)    (None, 32, 21, 256)  590080      conv4_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_bn (BatchNormali (None, 32, 21, 256)  1024        conv4_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_2_relu (Activation (None, 32, 21, 256)  0           conv4_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_conv (Conv2D)    (None, 32, 21, 1024) 525312      conv3_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_conv (Conv2D)    (None, 32, 21, 1024) 263168      conv4_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_0_bn (BatchNormali (None, 32, 21, 1024) 4096        conv4_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_3_bn (BatchNormali (None, 32, 21, 1024) 4096        conv4_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_add (Add)          (None, 32, 21, 1024) 0           conv4_block1_0_bn[0][0]          \n",
      "                                                                 conv4_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block1_out (Activation)   (None, 32, 21, 1024) 0           conv4_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_conv (Conv2D)    (None, 32, 21, 256)  262400      conv4_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_bn (BatchNormali (None, 32, 21, 256)  1024        conv4_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_1_relu (Activation (None, 32, 21, 256)  0           conv4_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_conv (Conv2D)    (None, 32, 21, 256)  590080      conv4_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_bn (BatchNormali (None, 32, 21, 256)  1024        conv4_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_2_relu (Activation (None, 32, 21, 256)  0           conv4_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_conv (Conv2D)    (None, 32, 21, 1024) 263168      conv4_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_3_bn (BatchNormali (None, 32, 21, 1024) 4096        conv4_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_add (Add)          (None, 32, 21, 1024) 0           conv4_block1_out[0][0]           \n",
      "                                                                 conv4_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block2_out (Activation)   (None, 32, 21, 1024) 0           conv4_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_conv (Conv2D)    (None, 32, 21, 256)  262400      conv4_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_bn (BatchNormali (None, 32, 21, 256)  1024        conv4_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_1_relu (Activation (None, 32, 21, 256)  0           conv4_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_conv (Conv2D)    (None, 32, 21, 256)  590080      conv4_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_bn (BatchNormali (None, 32, 21, 256)  1024        conv4_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_2_relu (Activation (None, 32, 21, 256)  0           conv4_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_conv (Conv2D)    (None, 32, 21, 1024) 263168      conv4_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_3_bn (BatchNormali (None, 32, 21, 1024) 4096        conv4_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_add (Add)          (None, 32, 21, 1024) 0           conv4_block2_out[0][0]           \n",
      "                                                                 conv4_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block3_out (Activation)   (None, 32, 21, 1024) 0           conv4_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_conv (Conv2D)    (None, 32, 21, 256)  262400      conv4_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_bn (BatchNormali (None, 32, 21, 256)  1024        conv4_block4_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_1_relu (Activation (None, 32, 21, 256)  0           conv4_block4_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_conv (Conv2D)    (None, 32, 21, 256)  590080      conv4_block4_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_bn (BatchNormali (None, 32, 21, 256)  1024        conv4_block4_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_2_relu (Activation (None, 32, 21, 256)  0           conv4_block4_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_conv (Conv2D)    (None, 32, 21, 1024) 263168      conv4_block4_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_3_bn (BatchNormali (None, 32, 21, 1024) 4096        conv4_block4_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_add (Add)          (None, 32, 21, 1024) 0           conv4_block3_out[0][0]           \n",
      "                                                                 conv4_block4_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block4_out (Activation)   (None, 32, 21, 1024) 0           conv4_block4_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_conv (Conv2D)    (None, 32, 21, 256)  262400      conv4_block4_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_bn (BatchNormali (None, 32, 21, 256)  1024        conv4_block5_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_1_relu (Activation (None, 32, 21, 256)  0           conv4_block5_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_conv (Conv2D)    (None, 32, 21, 256)  590080      conv4_block5_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_bn (BatchNormali (None, 32, 21, 256)  1024        conv4_block5_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_2_relu (Activation (None, 32, 21, 256)  0           conv4_block5_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_conv (Conv2D)    (None, 32, 21, 1024) 263168      conv4_block5_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_3_bn (BatchNormali (None, 32, 21, 1024) 4096        conv4_block5_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_add (Add)          (None, 32, 21, 1024) 0           conv4_block4_out[0][0]           \n",
      "                                                                 conv4_block5_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block5_out (Activation)   (None, 32, 21, 1024) 0           conv4_block5_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_conv (Conv2D)    (None, 32, 21, 256)  262400      conv4_block5_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_bn (BatchNormali (None, 32, 21, 256)  1024        conv4_block6_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_1_relu (Activation (None, 32, 21, 256)  0           conv4_block6_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_conv (Conv2D)    (None, 32, 21, 256)  590080      conv4_block6_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_bn (BatchNormali (None, 32, 21, 256)  1024        conv4_block6_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_2_relu (Activation (None, 32, 21, 256)  0           conv4_block6_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_conv (Conv2D)    (None, 32, 21, 1024) 263168      conv4_block6_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_3_bn (BatchNormali (None, 32, 21, 1024) 4096        conv4_block6_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_add (Add)          (None, 32, 21, 1024) 0           conv4_block5_out[0][0]           \n",
      "                                                                 conv4_block6_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv4_block6_out (Activation)   (None, 32, 21, 1024) 0           conv4_block6_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_conv (Conv2D)    (None, 16, 11, 512)  524800      conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_bn (BatchNormali (None, 16, 11, 512)  2048        conv5_block1_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_1_relu (Activation (None, 16, 11, 512)  0           conv5_block1_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_conv (Conv2D)    (None, 16, 11, 512)  2359808     conv5_block1_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_bn (BatchNormali (None, 16, 11, 512)  2048        conv5_block1_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_2_relu (Activation (None, 16, 11, 512)  0           conv5_block1_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_conv (Conv2D)    (None, 16, 11, 2048) 2099200     conv4_block6_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_conv (Conv2D)    (None, 16, 11, 2048) 1050624     conv5_block1_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_0_bn (BatchNormali (None, 16, 11, 2048) 8192        conv5_block1_0_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_3_bn (BatchNormali (None, 16, 11, 2048) 8192        conv5_block1_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_add (Add)          (None, 16, 11, 2048) 0           conv5_block1_0_bn[0][0]          \n",
      "                                                                 conv5_block1_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block1_out (Activation)   (None, 16, 11, 2048) 0           conv5_block1_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_conv (Conv2D)    (None, 16, 11, 512)  1049088     conv5_block1_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_bn (BatchNormali (None, 16, 11, 512)  2048        conv5_block2_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_1_relu (Activation (None, 16, 11, 512)  0           conv5_block2_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_conv (Conv2D)    (None, 16, 11, 512)  2359808     conv5_block2_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_bn (BatchNormali (None, 16, 11, 512)  2048        conv5_block2_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_2_relu (Activation (None, 16, 11, 512)  0           conv5_block2_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_conv (Conv2D)    (None, 16, 11, 2048) 1050624     conv5_block2_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_3_bn (BatchNormali (None, 16, 11, 2048) 8192        conv5_block2_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_add (Add)          (None, 16, 11, 2048) 0           conv5_block1_out[0][0]           \n",
      "                                                                 conv5_block2_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block2_out (Activation)   (None, 16, 11, 2048) 0           conv5_block2_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_conv (Conv2D)    (None, 16, 11, 512)  1049088     conv5_block2_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_bn (BatchNormali (None, 16, 11, 512)  2048        conv5_block3_1_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_1_relu (Activation (None, 16, 11, 512)  0           conv5_block3_1_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_conv (Conv2D)    (None, 16, 11, 512)  2359808     conv5_block3_1_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_bn (BatchNormali (None, 16, 11, 512)  2048        conv5_block3_2_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_2_relu (Activation (None, 16, 11, 512)  0           conv5_block3_2_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_conv (Conv2D)    (None, 16, 11, 2048) 1050624     conv5_block3_2_relu[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_3_bn (BatchNormali (None, 16, 11, 2048) 8192        conv5_block3_3_conv[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_add (Add)          (None, 16, 11, 2048) 0           conv5_block2_out[0][0]           \n",
      "                                                                 conv5_block3_3_bn[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "conv5_block3_out (Activation)   (None, 16, 11, 2048) 0           conv5_block3_add[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "avg_pool (GlobalAveragePooling2 (None, 2048)         0           conv5_block3_out[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "probs (Dense)                   (None, 20)           40980       avg_pool[0][0]                   \n",
      "==================================================================================================\n",
      "Total params: 23,628,692\n",
      "Trainable params: 23,575,572\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "tree = TreeCNN([(i+1) for i in range(20)], input_shape=(500, 335, 3))\n",
    "tree.root.net.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_base = 'CUB_200_2011/CUB_200_2011/'\n",
    "path_base = 'CUB_200_2011/' # IN DGX\n",
    "images_path_map = pd.read_csv(path_base + 'images.txt', sep=' ', header=None)\n",
    "\n",
    "images_path_map = images_path_map.values\n",
    "images_path_map[:,0] = images_path_map[:,0] - 1\n",
    "images_path_map\n",
    "\n",
    "map_path = {}\n",
    "for id_, path in images_path_map:\n",
    "    map_path[id_] = path_base + 'images/' + path\n",
    "map_path\n",
    "\n",
    "train_split = pd.read_csv(path_base + 'train_test_split.txt', sep=' ', header=None)\n",
    "train_mask = train_split.values[:,1]\n",
    "train_mask = train_mask==1\n",
    "\n",
    "imageId_label_map = pd.read_csv(path_base + 'image_class_labels.txt', sep=' ', header=None)\n",
    "imageId_label_map = imageId_label_map.values\n",
    "imageId_label_map[:, 0 ] = imageId_label_map[:, 0 ] -1\n",
    "\n",
    "imageId_label_map\n",
    "map_imageId_label = {}\n",
    "for id_, cls in imageId_label_map:\n",
    "    map_imageId_label[id_] = cls\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11788\n",
      "0\n",
      "100\n",
      "200\n",
      "300\n",
      "400\n",
      "500\n",
      "600\n",
      "700\n",
      "800\n",
      "900\n",
      "1000\n",
      "1100\n",
      "1200\n",
      "1300\n",
      "1400\n",
      "1500\n",
      "1600\n",
      "1700\n",
      "1800\n",
      "1900\n",
      "2000\n",
      "2100\n",
      "2200\n",
      "2300\n",
      "2400\n",
      "2500\n",
      "2600\n",
      "2700\n",
      "2800\n",
      "2900\n",
      "3000\n",
      "3100\n",
      "3200\n",
      "3300\n",
      "3400\n",
      "3500\n",
      "3600\n",
      "3700\n",
      "3800\n",
      "3900\n",
      "4000\n",
      "4100\n",
      "4200\n",
      "4300\n",
      "4400\n",
      "4500\n",
      "4600\n",
      "4700\n",
      "4800\n",
      "4900\n",
      "5000\n",
      "5100\n",
      "5200\n",
      "5300\n",
      "5400\n",
      "5500\n",
      "5600\n",
      "5700\n",
      "5800\n",
      "5900\n",
      "6000\n",
      "6100\n",
      "6200\n",
      "6300\n",
      "6400\n",
      "6500\n",
      "6600\n",
      "6700\n",
      "6800\n",
      "6900\n",
      "7000\n",
      "7100\n",
      "7200\n",
      "7300\n",
      "7400\n",
      "7500\n",
      "7600\n",
      "7700\n",
      "7800\n",
      "7900\n",
      "8000\n",
      "8100\n",
      "8200\n",
      "8300\n",
      "8400\n",
      "8500\n",
      "8600\n",
      "8700\n",
      "8800\n",
      "8900\n",
      "9000\n",
      "9100\n",
      "9200\n",
      "9300\n",
      "9400\n",
      "9500\n",
      "9600\n",
      "9700\n",
      "9800\n",
      "9900\n",
      "10000\n",
      "10100\n",
      "10200\n",
      "10300\n",
      "10400\n",
      "10500\n",
      "10600\n",
      "10700\n",
      "10800\n",
      "10900\n",
      "11000\n",
      "11100\n",
      "11200\n",
      "11300\n",
      "11400\n",
      "11500\n",
      "11600\n",
      "11700\n"
     ]
    }
   ],
   "source": [
    "sizes = (335, 500, 3)\n",
    "map_cv2_img = {}\n",
    "print(len(map_path.keys()))\n",
    "\n",
    "for idx in map_path.keys():\n",
    "    if idx%100==0:\n",
    "            print(idx)\n",
    "\n",
    "    img = cv2.imread(map_path[idx])/255.\n",
    "    img = cv2.resize(img, (sizes[0], sizes[1]))\n",
    "\n",
    "    map_cv2_img[idx] = img\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataset(num_classes = 20, start_at=1):\n",
    "    print('\\tCreating dataset...')\n",
    "    idx_used = (imageId_label_map[:,1] >= start_at) * (imageId_label_map[:,1] < (start_at+num_classes))\n",
    "    \n",
    "    imgs_id_to_dataset = imageId_label_map[idx_used][:, 0]\n",
    "    train_mask_used = train_mask[idx_used]\n",
    "    \n",
    "    dataset = []\n",
    "    sizes = (335, 500, 3)\n",
    "\n",
    "    X = []\n",
    "    Y = []\n",
    "    for idx in imgs_id_to_dataset:\n",
    "        if idx%100==0:\n",
    "            print('\\t\\t', idx,'/',len(imgs_id_to_dataset))\n",
    "        img = map_cv2_img[idx]\n",
    "        \n",
    "        X.append(img)\n",
    "        label = map_imageId_label[idx]\n",
    "        Y.append(label)\n",
    "    \n",
    "    print('\\t\\tConverting to numpy...')\n",
    "    X = np.array(X)\n",
    "    Y = np.array(Y)\n",
    "    \n",
    "    print('\\t\\tShuffle dataset')\n",
    "    shuffle_idxs = np.array([i for i in range(len(X))])\n",
    "    np.random.shuffle(shuffle_idxs)\n",
    "    for idx in range(len(shuffle_idxs)):\n",
    "        shuffle_idx = shuffle_idxs[idx]\n",
    "        X_aux = X[shuffle_idx]\n",
    "        X[shuffle_idx] = X[idx]\n",
    "        X[idx] = X_aux\n",
    "        Y_aux = Y[shuffle_idx]\n",
    "        Y[shuffle_idx] = Y[idx]\n",
    "        Y[idx] = Y_aux\n",
    "        \n",
    "    \n",
    "#     train_idx = train_mask_used==True\n",
    "#     test_idx = train_mask_used==False\n",
    "    print('\\t\\tSplitting in train and test')\n",
    "    return X[:int(X.shape[0]*.7)], Y[:int(X.shape[0]*.7)], X[int(X.shape[0]*.7):], Y[int(X.shape[0]*.7):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tCreating dataset...\n",
      "\t\t 0 / 60\n",
      "\t\tConverting to numpy...\n",
      "\t\tShuffle dataset\n",
      "\t\tSplitting in train and test\n",
      "(42, 500, 335, 3)\n",
      "(42,)\n",
      "(18, 500, 335, 3)\n",
      "(18,)\n",
      "1\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "# Validating method\n",
    "x_train, y_train, x_test, y_test = create_dataset(1)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_test.shape)\n",
    "print(len(np.unique(y_train)))\n",
    "print(len(np.unique(y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_after_retrain = []\n",
    "accuracy_before_retrain =[]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In iteration:  1\n",
      "\tCreating dataset...\n",
      "\t\t 0 / 1115\n",
      "\t\t 100 / 1115\n",
      "\t\t 200 / 1115\n",
      "\t\t 300 / 1115\n",
      "\t\t 400 / 1115\n",
      "\t\t 500 / 1115\n",
      "\t\t 600 / 1115\n",
      "\t\t 700 / 1115\n",
      "\t\t 800 / 1115\n",
      "\t\t 900 / 1115\n",
      "\t\t 1000 / 1115\n",
      "\t\t 1100 / 1115\n",
      "\t\tConverting to numpy...\n",
      "\t\tShuffle dataset\n",
      "\t\tSplitting in train and test\n",
      "\t * dataset created;\n",
      "\t * Evaluated before train ;\n",
      "\t * Trainning;\n",
      "1 {1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [7], 8: [8], 9: [9], 10: [10], 11: [11], 12: [12], 13: [13], 14: [14], 15: [15], 16: [16], 17: [17], 18: [18], 19: [19], 20: [20]}\n",
      "(780, 500, 335, 3) (780, 20)\n",
      "Train on 702 samples, validate on 78 samples\n",
      "Epoch 1/100000\n",
      "702/702 [==============================] - 18s 25ms/sample - loss: 4.0489 - accuracy: 0.0641 - val_loss: 3.0431 - val_accuracy: 0.0385\n",
      "Epoch 2/100000\n",
      "702/702 [==============================] - 9s 12ms/sample - loss: 3.3113 - accuracy: 0.0897 - val_loss: 3.0873 - val_accuracy: 0.0256\n",
      "Epoch 3/100000\n",
      "702/702 [==============================] - 9s 12ms/sample - loss: 3.1865 - accuracy: 0.0983 - val_loss: 3.1899 - val_accuracy: 0.0128\n",
      "Epoch 4/100000\n",
      "702/702 [==============================] - 9s 12ms/sample - loss: 3.0264 - accuracy: 0.1140 - val_loss: 3.2033 - val_accuracy: 0.0128\n",
      "Epoch 5/100000\n",
      "702/702 [==============================] - 9s 12ms/sample - loss: 2.8862 - accuracy: 0.1567 - val_loss: 3.1882 - val_accuracy: 0.0513\n",
      "Epoch 6/100000\n",
      "702/702 [==============================] - 9s 12ms/sample - loss: 2.8352 - accuracy: 0.1353 - val_loss: 3.3250 - val_accuracy: 0.0128\n",
      "Epoch 7/100000\n",
      "702/702 [==============================] - 9s 12ms/sample - loss: 2.8066 - accuracy: 0.1567 - val_loss: 3.2883 - val_accuracy: 0.0641\n",
      "Epoch 8/100000\n",
      "702/702 [==============================] - 9s 12ms/sample - loss: 2.7580 - accuracy: 0.1738 - val_loss: 3.4540 - val_accuracy: 0.0128\n",
      "Epoch 9/100000\n",
      "702/702 [==============================] - 9s 12ms/sample - loss: 2.7136 - accuracy: 0.1795 - val_loss: 3.2945 - val_accuracy: 0.0641\n",
      "Epoch 10/100000\n",
      "702/702 [==============================] - 9s 12ms/sample - loss: 2.6405 - accuracy: 0.1937 - val_loss: 3.3190 - val_accuracy: 0.1026\n",
      "Epoch 11/100000\n",
      "702/702 [==============================] - 9s 12ms/sample - loss: 2.6570 - accuracy: 0.1923 - val_loss: 3.7056 - val_accuracy: 0.0385\n",
      "Epoch 12/100000\n",
      "702/702 [==============================] - 9s 12ms/sample - loss: 2.6104 - accuracy: 0.2066 - val_loss: 3.5970 - val_accuracy: 0.0769\n",
      "Epoch 13/100000\n",
      "702/702 [==============================] - 9s 13ms/sample - loss: 2.5601 - accuracy: 0.2066 - val_loss: 3.6389 - val_accuracy: 0.0385\n",
      "Epoch 14/100000\n",
      "702/702 [==============================] - 9s 13ms/sample - loss: 2.5221 - accuracy: 0.2137 - val_loss: 4.0433 - val_accuracy: 0.0513\n",
      "Epoch 15/100000\n",
      "702/702 [==============================] - 9s 12ms/sample - loss: 2.5557 - accuracy: 0.2094 - val_loss: 4.4267 - val_accuracy: 0.0256\n",
      "Epoch 16/100000\n",
      "702/702 [==============================] - 9s 12ms/sample - loss: 2.4589 - accuracy: 0.2635 - val_loss: 3.7248 - val_accuracy: 0.0513\n",
      "\t * Evaluated after train ;\n",
      "In iteration:  2\n",
      "\tCreating dataset...\n",
      "\t\t 0 / 2290\n",
      "\t\t 100 / 2290\n",
      "\t\t 200 / 2290\n",
      "\t\t 300 / 2290\n",
      "\t\t 400 / 2290\n",
      "\t\t 500 / 2290\n",
      "\t\t 600 / 2290\n",
      "\t\t 700 / 2290\n",
      "\t\t 800 / 2290\n",
      "\t\t 900 / 2290\n",
      "\t\t 1000 / 2290\n",
      "\t\t 1100 / 2290\n",
      "\t\t 1200 / 2290\n",
      "\t\t 1300 / 2290\n",
      "\t\t 1400 / 2290\n",
      "\t\t 1500 / 2290\n",
      "\t\t 1600 / 2290\n",
      "\t\t 1700 / 2290\n",
      "\t\t 1800 / 2290\n",
      "\t\t 1900 / 2290\n",
      "\t\t 2000 / 2290\n",
      "\t\t 2100 / 2290\n",
      "\t\t 2200 / 2290\n",
      "\t\tConverting to numpy...\n",
      "\t\tShuffle dataset\n",
      "\t\tSplitting in train and test\n",
      "\t * dataset created;\n",
      "\t * Evaluated before train ;\n",
      "\t * Adding new classes ;\n",
      "\tCreating dataset...\n",
      "\t\t 1200 / 1175\n",
      "\t\t 1300 / 1175\n",
      "\t\t 1400 / 1175\n",
      "\t\t 1500 / 1175\n",
      "\t\t 1600 / 1175\n",
      "\t\t 1700 / 1175\n",
      "\t\t 1800 / 1175\n",
      "\t\t 1900 / 1175\n",
      "\t\t 2000 / 1175\n",
      "\t\t 2100 / 1175\n",
      "\t\t 2200 / 1175\n",
      "\t\tConverting to numpy...\n",
      "\t\tShuffle dataset\n",
      "\t\tSplitting in train and test\n",
      "\t * Trainning;\n",
      "1 {1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [7], 8: [8], 9: [9], 10: [10], 11: [11], 12: [12], 13: [13], 14: [14], 15: [15], 16: [16], 17: [17], 18: [18], 19: [19], 20: [20], 34: [34], 38: [38], 27: [27], 32: [32], 33: [33], 28: [28], 26: [26], 21: [21], 37: [37], 40: [40], 36: [36], 30: [30], 35: [35], 29: [29], 31: [31], 39: [39], 24: [24], 25: [25], 22: [22], 23: [23]}\n",
      "(1603, 500, 335, 3) (1603, 40)\n",
      "Train on 1442 samples, validate on 161 samples\n",
      "Epoch 1/100000\n",
      "1442/1442 [==============================] - 26s 18ms/sample - loss: 3.5110 - accuracy: 0.1297 - val_loss: 4.4685 - val_accuracy: 0.0311\n",
      "Epoch 2/100000\n",
      "1442/1442 [==============================] - 18s 12ms/sample - loss: 3.3635 - accuracy: 0.1519 - val_loss: 4.5037 - val_accuracy: 0.0559\n",
      "Epoch 3/100000\n",
      "1442/1442 [==============================] - 18s 12ms/sample - loss: 3.3015 - accuracy: 0.1623 - val_loss: 7.8831 - val_accuracy: 0.0186\n",
      "Epoch 4/100000\n",
      "1442/1442 [==============================] - 18s 12ms/sample - loss: 3.2483 - accuracy: 0.1588 - val_loss: 4.4021 - val_accuracy: 0.0621\n",
      "Epoch 5/100000\n",
      "1442/1442 [==============================] - 18s 12ms/sample - loss: 3.1848 - accuracy: 0.1824 - val_loss: 4.1433 - val_accuracy: 0.0559\n",
      "Epoch 6/100000\n",
      "1442/1442 [==============================] - 18s 12ms/sample - loss: 3.1562 - accuracy: 0.1838 - val_loss: 6.0695 - val_accuracy: 0.0248\n",
      "Epoch 7/100000\n",
      "1442/1442 [==============================] - 18s 12ms/sample - loss: 3.1175 - accuracy: 0.2025 - val_loss: 5.0490 - val_accuracy: 0.0124\n",
      "Epoch 8/100000\n",
      "1442/1442 [==============================] - 18s 12ms/sample - loss: 3.0669 - accuracy: 0.2053 - val_loss: 5.6712 - val_accuracy: 0.0497\n",
      "Epoch 9/100000\n",
      "1442/1442 [==============================] - 18s 12ms/sample - loss: 3.0524 - accuracy: 0.2136 - val_loss: 4.5941 - val_accuracy: 0.0683\n",
      "Epoch 10/100000\n",
      "1442/1442 [==============================] - 18s 12ms/sample - loss: 2.9956 - accuracy: 0.2087 - val_loss: 6.5232 - val_accuracy: 0.0435\n",
      "Epoch 11/100000\n",
      "1442/1442 [==============================] - 18s 12ms/sample - loss: 2.9552 - accuracy: 0.2254 - val_loss: 5.9925 - val_accuracy: 0.0497\n",
      "Epoch 12/100000\n",
      "1442/1442 [==============================] - 18s 12ms/sample - loss: 2.8711 - accuracy: 0.2503 - val_loss: 4.6178 - val_accuracy: 0.0435\n",
      "Epoch 13/100000\n",
      "1442/1442 [==============================] - 18s 12ms/sample - loss: 2.8815 - accuracy: 0.2448 - val_loss: 5.0357 - val_accuracy: 0.0745\n",
      "Epoch 14/100000\n",
      "1442/1442 [==============================] - 18s 12ms/sample - loss: 2.8491 - accuracy: 0.2413 - val_loss: 4.9258 - val_accuracy: 0.0373\n",
      "Epoch 15/100000\n",
      "1442/1442 [==============================] - 18s 12ms/sample - loss: 2.7891 - accuracy: 0.2531 - val_loss: 5.0698 - val_accuracy: 0.0683\n",
      "Epoch 16/100000\n",
      "1442/1442 [==============================] - 18s 12ms/sample - loss: 2.7486 - accuracy: 0.2559 - val_loss: 6.8180 - val_accuracy: 0.0124\n",
      "Epoch 17/100000\n",
      "1442/1442 [==============================] - 18s 12ms/sample - loss: 2.6964 - accuracy: 0.2781 - val_loss: 5.6576 - val_accuracy: 0.0248\n",
      "Epoch 18/100000\n",
      "1442/1442 [==============================] - 18s 12ms/sample - loss: 2.6432 - accuracy: 0.2878 - val_loss: 6.2308 - val_accuracy: 0.0373\n",
      "Epoch 19/100000\n",
      "1442/1442 [==============================] - 18s 12ms/sample - loss: 2.6571 - accuracy: 0.2926 - val_loss: 6.0764 - val_accuracy: 0.0311\n",
      "Epoch 20/100000\n",
      "1442/1442 [==============================] - 18s 12ms/sample - loss: 2.6336 - accuracy: 0.2802 - val_loss: 7.5098 - val_accuracy: 0.0745\n",
      "\t * Evaluated after train ;\n",
      "In iteration:  3\n",
      "\tCreating dataset...\n",
      "\t\t 0 / 3486\n",
      "\t\t 100 / 3486\n",
      "\t\t 200 / 3486\n",
      "\t\t 300 / 3486\n",
      "\t\t 400 / 3486\n",
      "\t\t 500 / 3486\n",
      "\t\t 600 / 3486\n",
      "\t\t 700 / 3486\n",
      "\t\t 800 / 3486\n",
      "\t\t 900 / 3486\n",
      "\t\t 1000 / 3486\n",
      "\t\t 1100 / 3486\n",
      "\t\t 1200 / 3486\n",
      "\t\t 1300 / 3486\n",
      "\t\t 1400 / 3486\n",
      "\t\t 1500 / 3486\n",
      "\t\t 1600 / 3486\n",
      "\t\t 1700 / 3486\n",
      "\t\t 1800 / 3486\n",
      "\t\t 1900 / 3486\n",
      "\t\t 2000 / 3486\n",
      "\t\t 2100 / 3486\n",
      "\t\t 2200 / 3486\n",
      "\t\t 2300 / 3486\n",
      "\t\t 2400 / 3486\n",
      "\t\t 2500 / 3486\n",
      "\t\t 2600 / 3486\n",
      "\t\t 2700 / 3486\n",
      "\t\t 2800 / 3486\n",
      "\t\t 2900 / 3486\n",
      "\t\t 3000 / 3486\n",
      "\t\t 3100 / 3486\n",
      "\t\t 3200 / 3486\n",
      "\t\t 3300 / 3486\n",
      "\t\t 3400 / 3486\n",
      "\t\tConverting to numpy...\n",
      "\t\tShuffle dataset\n",
      "\t\tSplitting in train and test\n",
      "\t * dataset created;\n",
      "\t * Evaluated before train ;\n",
      "\t * Adding new classes ;\n",
      "\tCreating dataset...\n",
      "\t\t 2300 / 1196\n",
      "\t\t 2400 / 1196\n",
      "\t\t 2500 / 1196\n",
      "\t\t 2600 / 1196\n",
      "\t\t 2700 / 1196\n",
      "\t\t 2800 / 1196\n",
      "\t\t 2900 / 1196\n",
      "\t\t 3000 / 1196\n",
      "\t\t 3100 / 1196\n",
      "\t\t 3200 / 1196\n",
      "\t\t 3300 / 1196\n",
      "\t\t 3400 / 1196\n",
      "\t\tConverting to numpy...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tShuffle dataset\n",
      "\t\tSplitting in train and test\n",
      "\t * Trainning;\n",
      "1 {1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [7], 8: [8], 9: [9], 10: [10], 11: [11], 12: [12], 13: [13], 14: [14], 15: [15], 16: [16], 17: [17], 18: [18], 19: [19], 20: [20], 34: [34], 38: [38], 27: [27], 32: [32], 33: [33], 28: [28], 26: [26], 21: [21], 37: [37], 40: [40], 36: [36], 30: [30], 35: [35], 29: [29], 31: [31], 39: [39], 24: [24], 25: [25], 22: [22], 23: [23], 43: [43], 41: [41], 44: [44], 54: [54], 45: [45], 52: [52], 60: [60], 46: [46], 49: [49], 50: [50], 53: [53], 59: [59], 55: [55], 47: [47], 58: [58], 51: [51], 56: [56], 48: [48], 57: [57], 42: [42]}\n",
      "(2440, 500, 335, 3) (2440, 60)\n",
      "Train on 2196 samples, validate on 244 samples\n",
      "Epoch 1/100000\n",
      "2196/2196 [==============================] - 35s 16ms/sample - loss: 3.6427 - accuracy: 0.1717 - val_loss: 5.6605 - val_accuracy: 0.0369\n",
      "Epoch 2/100000\n",
      "2196/2196 [==============================] - 27s 12ms/sample - loss: 3.4617 - accuracy: 0.1867 - val_loss: 5.4558 - val_accuracy: 0.0738\n",
      "Epoch 3/100000\n",
      "2196/2196 [==============================] - 27s 12ms/sample - loss: 3.3534 - accuracy: 0.2058 - val_loss: 4.8211 - val_accuracy: 0.0574\n",
      "Epoch 4/100000\n",
      "2196/2196 [==============================] - 27s 12ms/sample - loss: 3.3023 - accuracy: 0.2054 - val_loss: 5.9355 - val_accuracy: 0.0246\n",
      "Epoch 5/100000\n",
      "2196/2196 [==============================] - 27s 12ms/sample - loss: 3.2287 - accuracy: 0.2181 - val_loss: 5.0800 - val_accuracy: 0.0902\n",
      "Epoch 6/100000\n",
      "2196/2196 [==============================] - 27s 12ms/sample - loss: 3.1803 - accuracy: 0.2400 - val_loss: 6.0790 - val_accuracy: 0.0492\n",
      "Epoch 7/100000\n",
      "2196/2196 [==============================] - 27s 12ms/sample - loss: 3.1396 - accuracy: 0.2391 - val_loss: 4.9758 - val_accuracy: 0.0656\n",
      "Epoch 8/100000\n",
      "2196/2196 [==============================] - 27s 12ms/sample - loss: 3.0583 - accuracy: 0.2541 - val_loss: 9.8568 - val_accuracy: 0.0492\n",
      "Epoch 9/100000\n",
      "2196/2196 [==============================] - 27s 12ms/sample - loss: 3.0356 - accuracy: 0.2582 - val_loss: 6.4811 - val_accuracy: 0.0492\n",
      "Epoch 10/100000\n",
      "2196/2196 [==============================] - 27s 12ms/sample - loss: 2.9327 - accuracy: 0.2764 - val_loss: 6.7108 - val_accuracy: 0.0574\n",
      "Epoch 11/100000\n",
      "2196/2196 [==============================] - 27s 12ms/sample - loss: 2.8947 - accuracy: 0.2805 - val_loss: 6.2235 - val_accuracy: 0.0615\n",
      "Epoch 12/100000\n",
      "2196/2196 [==============================] - 27s 12ms/sample - loss: 2.8126 - accuracy: 0.3074 - val_loss: 6.0888 - val_accuracy: 0.0656\n",
      "Epoch 13/100000\n",
      "2196/2196 [==============================] - 27s 12ms/sample - loss: 2.7651 - accuracy: 0.3074 - val_loss: 7.0672 - val_accuracy: 0.0451\n",
      "Epoch 14/100000\n",
      "2196/2196 [==============================] - 27s 12ms/sample - loss: 2.7574 - accuracy: 0.3078 - val_loss: 7.0393 - val_accuracy: 0.0287\n",
      "Epoch 15/100000\n",
      "2196/2196 [==============================] - 27s 12ms/sample - loss: 2.7037 - accuracy: 0.3160 - val_loss: 5.6424 - val_accuracy: 0.0328\n",
      "Epoch 16/100000\n",
      "2196/2196 [==============================] - 27s 12ms/sample - loss: 2.6229 - accuracy: 0.3279 - val_loss: 7.3337 - val_accuracy: 0.0861\n",
      "Epoch 17/100000\n",
      "2196/2196 [==============================] - 27s 12ms/sample - loss: 2.5395 - accuracy: 0.3474 - val_loss: 6.2902 - val_accuracy: 0.0369\n",
      "Epoch 18/100000\n",
      "2196/2196 [==============================] - 27s 12ms/sample - loss: 2.5182 - accuracy: 0.3520 - val_loss: 8.9288 - val_accuracy: 0.0369\n",
      "\t * Evaluated after train ;\n",
      "In iteration:  4\n",
      "\tCreating dataset...\n",
      "\t\t 0 / 4672\n",
      "\t\t 100 / 4672\n",
      "\t\t 200 / 4672\n",
      "\t\t 300 / 4672\n",
      "\t\t 400 / 4672\n",
      "\t\t 500 / 4672\n",
      "\t\t 600 / 4672\n",
      "\t\t 700 / 4672\n",
      "\t\t 800 / 4672\n",
      "\t\t 900 / 4672\n",
      "\t\t 1000 / 4672\n",
      "\t\t 1100 / 4672\n",
      "\t\t 1200 / 4672\n",
      "\t\t 1300 / 4672\n",
      "\t\t 1400 / 4672\n",
      "\t\t 1500 / 4672\n",
      "\t\t 1600 / 4672\n",
      "\t\t 1700 / 4672\n",
      "\t\t 1800 / 4672\n",
      "\t\t 1900 / 4672\n",
      "\t\t 2000 / 4672\n",
      "\t\t 2100 / 4672\n",
      "\t\t 2200 / 4672\n",
      "\t\t 2300 / 4672\n",
      "\t\t 2400 / 4672\n",
      "\t\t 2500 / 4672\n",
      "\t\t 2600 / 4672\n",
      "\t\t 2700 / 4672\n",
      "\t\t 2800 / 4672\n",
      "\t\t 2900 / 4672\n",
      "\t\t 3000 / 4672\n",
      "\t\t 3100 / 4672\n",
      "\t\t 3200 / 4672\n",
      "\t\t 3300 / 4672\n",
      "\t\t 3400 / 4672\n",
      "\t\t 3500 / 4672\n",
      "\t\t 3600 / 4672\n",
      "\t\t 3700 / 4672\n",
      "\t\t 3800 / 4672\n",
      "\t\t 3900 / 4672\n",
      "\t\t 4000 / 4672\n",
      "\t\t 4100 / 4672\n",
      "\t\t 4200 / 4672\n",
      "\t\t 4300 / 4672\n",
      "\t\t 4400 / 4672\n",
      "\t\t 4500 / 4672\n",
      "\t\t 4600 / 4672\n",
      "\t\tConverting to numpy...\n",
      "\t\tShuffle dataset\n",
      "\t\tSplitting in train and test\n",
      "\t * dataset created;\n",
      "\t * Evaluated before train ;\n",
      "\t * Adding new classes ;\n",
      "\tCreating dataset...\n",
      "\t\t 3500 / 1186\n",
      "\t\t 3600 / 1186\n",
      "\t\t 3700 / 1186\n",
      "\t\t 3800 / 1186\n",
      "\t\t 3900 / 1186\n",
      "\t\t 4000 / 1186\n",
      "\t\t 4100 / 1186\n",
      "\t\t 4200 / 1186\n",
      "\t\t 4300 / 1186\n",
      "\t\t 4400 / 1186\n",
      "\t\t 4500 / 1186\n",
      "\t\t 4600 / 1186\n",
      "\t\tConverting to numpy...\n",
      "\t\tShuffle dataset\n",
      "\t\tSplitting in train and test\n",
      "\t * Trainning;\n",
      "1 {1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [7], 8: [8], 9: [9], 10: [10], 11: [11], 12: [12], 13: [13], 14: [14], 15: [15], 16: [16], 17: [17], 18: [18], 19: [19], 20: [20], 34: [34], 38: [38], 27: [27], 32: [32], 33: [33], 28: [28], 26: [26], 21: [21], 37: [37], 40: [40], 36: [36], 30: [30], 35: [35], 29: [29], 31: [31], 39: [39], 24: [24], 25: [25], 22: [22], 23: [23], 43: [43], 41: [41], 44: [44], 54: [54], 45: [45], 52: [52], 60: [60], 46: [46], 49: [49], 50: [50], 53: [53], 59: [59], 55: [55], 47: [47], 58: [58], 51: [51], 56: [56], 48: [48], 57: [57], 42: [42], 77: [77], 64: [64], 74: [74], 71: [71], 80: [80], 73: [73], 63: [63], 62: [62], 78: [78], 65: [65], 75: [75], 72: [72], 67: [67], 70: [70], 79: [79], 76: [76], 66: [66], 69: [69], 68: [68], 61: [61]}\n",
      "(3270, 500, 335, 3) (3270, 80)\n",
      "Train on 2943 samples, validate on 327 samples\n",
      "Epoch 1/100000\n",
      "2943/2943 [==============================] - 44s 15ms/sample - loss: 3.7201 - accuracy: 0.2171 - val_loss: 5.0149 - val_accuracy: 0.1040\n",
      "Epoch 2/100000\n",
      "2943/2943 [==============================] - 35s 12ms/sample - loss: 3.4263 - accuracy: 0.2562 - val_loss: 4.7258 - val_accuracy: 0.1009\n",
      "Epoch 3/100000\n",
      "2943/2943 [==============================] - 36s 12ms/sample - loss: 3.2653 - accuracy: 0.2749 - val_loss: 4.9017 - val_accuracy: 0.1009\n",
      "Epoch 4/100000\n",
      "2943/2943 [==============================] - 36s 12ms/sample - loss: 3.2130 - accuracy: 0.2756 - val_loss: 5.5151 - val_accuracy: 0.0734\n",
      "Epoch 5/100000\n",
      "2943/2943 [==============================] - 36s 12ms/sample - loss: 3.1265 - accuracy: 0.2932 - val_loss: 4.9112 - val_accuracy: 0.0765\n",
      "Epoch 6/100000\n",
      "2943/2943 [==============================] - 36s 12ms/sample - loss: 2.9939 - accuracy: 0.3238 - val_loss: 5.1004 - val_accuracy: 0.0795\n",
      "Epoch 7/100000\n",
      "2943/2943 [==============================] - 36s 12ms/sample - loss: 2.9271 - accuracy: 0.3347 - val_loss: 6.2425 - val_accuracy: 0.0550\n",
      "Epoch 8/100000\n",
      "2943/2943 [==============================] - 36s 12ms/sample - loss: 2.8323 - accuracy: 0.3459 - val_loss: 6.4297 - val_accuracy: 0.0550\n",
      "Epoch 9/100000\n",
      "2943/2943 [==============================] - 36s 12ms/sample - loss: 2.7357 - accuracy: 0.3687 - val_loss: 5.9517 - val_accuracy: 0.0581\n",
      "Epoch 10/100000\n",
      "2943/2943 [==============================] - 36s 12ms/sample - loss: 2.6780 - accuracy: 0.3738 - val_loss: 6.3442 - val_accuracy: 0.0581\n",
      "Epoch 11/100000\n",
      "2943/2943 [==============================] - 36s 12ms/sample - loss: 2.5547 - accuracy: 0.3911 - val_loss: 5.7020 - val_accuracy: 0.0917\n",
      "Epoch 12/100000\n",
      "2943/2943 [==============================] - 36s 12ms/sample - loss: 2.4638 - accuracy: 0.4122 - val_loss: 5.7027 - val_accuracy: 0.0520\n",
      "Epoch 13/100000\n",
      "2943/2943 [==============================] - 36s 12ms/sample - loss: 2.3961 - accuracy: 0.4128 - val_loss: 5.7199 - val_accuracy: 0.0642\n",
      "Epoch 14/100000\n",
      "2943/2943 [==============================] - 36s 12ms/sample - loss: 2.2726 - accuracy: 0.4427 - val_loss: 5.9709 - val_accuracy: 0.0612\n",
      "Epoch 15/100000\n",
      "2943/2943 [==============================] - 36s 12ms/sample - loss: 2.2295 - accuracy: 0.4472 - val_loss: 7.5954 - val_accuracy: 0.0336\n",
      "Epoch 16/100000\n",
      "2943/2943 [==============================] - 36s 12ms/sample - loss: 2.1389 - accuracy: 0.4618 - val_loss: 7.1483 - val_accuracy: 0.0795\n",
      "Epoch 17/100000\n",
      "2943/2943 [==============================] - 36s 12ms/sample - loss: 2.0559 - accuracy: 0.4798 - val_loss: 6.2575 - val_accuracy: 0.0489\n",
      "\t * Evaluated after train ;\n",
      "In iteration:  5\n",
      "\tCreating dataset...\n",
      "\t\t 0 / 5864\n",
      "\t\t 100 / 5864\n",
      "\t\t 200 / 5864\n",
      "\t\t 300 / 5864\n",
      "\t\t 400 / 5864\n",
      "\t\t 500 / 5864\n",
      "\t\t 600 / 5864\n",
      "\t\t 700 / 5864\n",
      "\t\t 800 / 5864\n",
      "\t\t 900 / 5864\n",
      "\t\t 1000 / 5864\n",
      "\t\t 1100 / 5864\n",
      "\t\t 1200 / 5864\n",
      "\t\t 1300 / 5864\n",
      "\t\t 1400 / 5864\n",
      "\t\t 1500 / 5864\n",
      "\t\t 1600 / 5864\n",
      "\t\t 1700 / 5864\n",
      "\t\t 1800 / 5864\n",
      "\t\t 1900 / 5864\n",
      "\t\t 2000 / 5864\n",
      "\t\t 2100 / 5864\n",
      "\t\t 2200 / 5864\n",
      "\t\t 2300 / 5864\n",
      "\t\t 2400 / 5864\n",
      "\t\t 2500 / 5864\n",
      "\t\t 2600 / 5864\n",
      "\t\t 2700 / 5864\n",
      "\t\t 2800 / 5864\n",
      "\t\t 2900 / 5864\n",
      "\t\t 3000 / 5864\n",
      "\t\t 3100 / 5864\n",
      "\t\t 3200 / 5864\n",
      "\t\t 3300 / 5864\n",
      "\t\t 3400 / 5864\n",
      "\t\t 3500 / 5864\n",
      "\t\t 3600 / 5864\n",
      "\t\t 3700 / 5864\n",
      "\t\t 3800 / 5864\n",
      "\t\t 3900 / 5864\n",
      "\t\t 4000 / 5864\n",
      "\t\t 4100 / 5864\n",
      "\t\t 4200 / 5864\n",
      "\t\t 4300 / 5864\n",
      "\t\t 4400 / 5864\n",
      "\t\t 4500 / 5864\n",
      "\t\t 4600 / 5864\n",
      "\t\t 4700 / 5864\n",
      "\t\t 4800 / 5864\n",
      "\t\t 4900 / 5864\n",
      "\t\t 5000 / 5864\n",
      "\t\t 5100 / 5864\n",
      "\t\t 5200 / 5864\n",
      "\t\t 5300 / 5864\n",
      "\t\t 5400 / 5864\n",
      "\t\t 5500 / 5864\n",
      "\t\t 5600 / 5864\n",
      "\t\t 5700 / 5864\n",
      "\t\t 5800 / 5864\n",
      "\t\tConverting to numpy...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t\tShuffle dataset\n",
      "\t\tSplitting in train and test\n",
      "\t * dataset created;\n",
      "\t * Evaluated before train ;\n",
      "\t * Adding new classes ;\n",
      "\tCreating dataset...\n",
      "\t\t 4700 / 1192\n",
      "\t\t 4800 / 1192\n",
      "\t\t 4900 / 1192\n",
      "\t\t 5000 / 1192\n",
      "\t\t 5100 / 1192\n",
      "\t\t 5200 / 1192\n",
      "\t\t 5300 / 1192\n",
      "\t\t 5400 / 1192\n",
      "\t\t 5500 / 1192\n",
      "\t\t 5600 / 1192\n",
      "\t\t 5700 / 1192\n",
      "\t\t 5800 / 1192\n",
      "\t\tConverting to numpy...\n",
      "\t\tShuffle dataset\n",
      "\t\tSplitting in train and test\n",
      "\t * Trainning;\n",
      "1 {1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [7], 8: [8], 9: [9], 10: [10], 11: [11], 12: [12], 13: [13], 14: [14], 15: [15], 16: [16], 17: [17], 18: [18], 19: [19], 20: [20], 34: [34], 38: [38], 27: [27], 32: [32], 33: [33], 28: [28], 26: [26], 21: [21], 37: [37], 40: [40], 36: [36], 30: [30], 35: [35], 29: [29], 31: [31], 39: [39], 24: [24], 25: [25], 22: [22], 23: [23], 43: [43], 41: [41], 44: [44], 54: [54], 45: [45], 52: [52], 60: [60], 46: [46], 49: [49], 50: [50], 53: [53], 59: [59], 55: [55], 47: [47], 58: [58], 51: [51], 56: [56], 48: [48], 57: [57], 42: [42], 77: [77], 64: [64], 74: [74], 71: [71], 80: [80], 73: [73], 63: [63], 62: [62], 78: [78], 65: [65], 75: [75], 72: [72], 67: [67], 70: [70], 79: [79], 76: [76], 66: [66], 69: [69], 68: [68], 61: [61], 81: [81], 85: [85], 86: [86], 99: [99], 92: [92], 91: [91], 94: [94], 100: [100], 93: [93], 97: [97], 89: [89], 90: [90], 82: [82], 95: [95], 87: [87], 84: [84], 98: [98], 88: [88], 96: [96], 83: [83]}\n",
      "(4104, 500, 335, 3) (4104, 100)\n",
      "Train on 3693 samples, validate on 411 samples\n",
      "Epoch 1/100000\n",
      "3693/3693 [==============================] - 53s 14ms/sample - loss: 3.6884 - accuracy: 0.2784 - val_loss: 5.4496 - val_accuracy: 0.1095\n",
      "Epoch 2/100000\n",
      "3693/3693 [==============================] - 45s 12ms/sample - loss: 3.3281 - accuracy: 0.3239 - val_loss: 4.8232 - val_accuracy: 0.1071\n",
      "Epoch 3/100000\n",
      "3693/3693 [==============================] - 45s 12ms/sample - loss: 3.1430 - accuracy: 0.3477 - val_loss: 5.4206 - val_accuracy: 0.0852\n",
      "Epoch 4/100000\n",
      "3693/3693 [==============================] - 45s 12ms/sample - loss: 2.9354 - accuracy: 0.3907 - val_loss: 7.0231 - val_accuracy: 0.0730\n",
      "Epoch 5/100000\n",
      "3693/3693 [==============================] - 45s 12ms/sample - loss: 2.7785 - accuracy: 0.4064 - val_loss: 6.7357 - val_accuracy: 0.1046\n",
      "Epoch 6/100000\n",
      "3693/3693 [==============================] - 45s 12ms/sample - loss: 2.6766 - accuracy: 0.4230 - val_loss: 5.2078 - val_accuracy: 0.0803\n",
      "Epoch 7/100000\n",
      "3693/3693 [==============================] - 45s 12ms/sample - loss: 2.5573 - accuracy: 0.4408 - val_loss: 5.4920 - val_accuracy: 0.0730\n",
      "Epoch 8/100000\n",
      "3693/3693 [==============================] - 45s 12ms/sample - loss: 2.4142 - accuracy: 0.4541 - val_loss: 6.1413 - val_accuracy: 0.0973\n",
      "Epoch 9/100000\n",
      "3693/3693 [==============================] - 44s 12ms/sample - loss: 2.3046 - accuracy: 0.4790 - val_loss: 6.3304 - val_accuracy: 0.0584\n",
      "Epoch 10/100000\n",
      "3693/3693 [==============================] - 45s 12ms/sample - loss: 2.2032 - accuracy: 0.4869 - val_loss: 8.2960 - val_accuracy: 0.0292\n",
      "Epoch 11/100000\n",
      "3693/3693 [==============================] - 45s 12ms/sample - loss: 2.1021 - accuracy: 0.5083 - val_loss: 6.3248 - val_accuracy: 0.0365\n",
      "Epoch 12/100000\n",
      "3693/3693 [==============================] - 45s 12ms/sample - loss: 2.0418 - accuracy: 0.5164 - val_loss: 5.8122 - val_accuracy: 0.0584\n",
      "Epoch 13/100000\n",
      "3693/3693 [==============================] - 45s 12ms/sample - loss: 1.9249 - accuracy: 0.5359 - val_loss: 7.2359 - val_accuracy: 0.0462\n",
      "Epoch 14/100000\n",
      "3693/3693 [==============================] - 45s 12ms/sample - loss: 1.8763 - accuracy: 0.5421 - val_loss: 6.8136 - val_accuracy: 0.0487\n",
      "Epoch 15/100000\n",
      "3693/3693 [==============================] - 45s 12ms/sample - loss: 1.8339 - accuracy: 0.5462 - val_loss: 6.2999 - val_accuracy: 0.0511\n",
      "Epoch 16/100000\n",
      "3693/3693 [==============================] - 45s 12ms/sample - loss: 1.7536 - accuracy: 0.5640 - val_loss: 12.8263 - val_accuracy: 0.0462\n",
      "Epoch 17/100000\n",
      "3693/3693 [==============================] - 45s 12ms/sample - loss: 1.7036 - accuracy: 0.5535 - val_loss: 7.8630 - val_accuracy: 0.0560\n",
      "\t * Evaluated after train ;\n",
      "In iteration:  6\n",
      "\tCreating dataset...\n",
      "\t\t 0 / 7029\n",
      "\t\t 100 / 7029\n",
      "\t\t 200 / 7029\n",
      "\t\t 300 / 7029\n",
      "\t\t 400 / 7029\n",
      "\t\t 500 / 7029\n",
      "\t\t 600 / 7029\n",
      "\t\t 700 / 7029\n",
      "\t\t 800 / 7029\n",
      "\t\t 900 / 7029\n",
      "\t\t 1000 / 7029\n",
      "\t\t 1100 / 7029\n",
      "\t\t 1200 / 7029\n",
      "\t\t 1300 / 7029\n",
      "\t\t 1400 / 7029\n",
      "\t\t 1500 / 7029\n",
      "\t\t 1600 / 7029\n",
      "\t\t 1700 / 7029\n",
      "\t\t 1800 / 7029\n",
      "\t\t 1900 / 7029\n",
      "\t\t 2000 / 7029\n",
      "\t\t 2100 / 7029\n",
      "\t\t 2200 / 7029\n",
      "\t\t 2300 / 7029\n",
      "\t\t 2400 / 7029\n",
      "\t\t 2500 / 7029\n",
      "\t\t 2600 / 7029\n",
      "\t\t 2700 / 7029\n",
      "\t\t 2800 / 7029\n",
      "\t\t 2900 / 7029\n",
      "\t\t 3000 / 7029\n",
      "\t\t 3100 / 7029\n",
      "\t\t 3200 / 7029\n",
      "\t\t 3300 / 7029\n",
      "\t\t 3400 / 7029\n",
      "\t\t 3500 / 7029\n",
      "\t\t 3600 / 7029\n",
      "\t\t 3700 / 7029\n",
      "\t\t 3800 / 7029\n",
      "\t\t 3900 / 7029\n",
      "\t\t 4000 / 7029\n",
      "\t\t 4100 / 7029\n",
      "\t\t 4200 / 7029\n",
      "\t\t 4300 / 7029\n",
      "\t\t 4400 / 7029\n",
      "\t\t 4500 / 7029\n",
      "\t\t 4600 / 7029\n",
      "\t\t 4700 / 7029\n",
      "\t\t 4800 / 7029\n",
      "\t\t 4900 / 7029\n",
      "\t\t 5000 / 7029\n",
      "\t\t 5100 / 7029\n",
      "\t\t 5200 / 7029\n",
      "\t\t 5300 / 7029\n",
      "\t\t 5400 / 7029\n",
      "\t\t 5500 / 7029\n",
      "\t\t 5600 / 7029\n",
      "\t\t 5700 / 7029\n",
      "\t\t 5800 / 7029\n",
      "\t\t 5900 / 7029\n",
      "\t\t 6000 / 7029\n",
      "\t\t 6100 / 7029\n",
      "\t\t 6200 / 7029\n",
      "\t\t 6300 / 7029\n",
      "\t\t 6400 / 7029\n",
      "\t\t 6500 / 7029\n",
      "\t\t 6600 / 7029\n",
      "\t\t 6700 / 7029\n",
      "\t\t 6800 / 7029\n",
      "\t\t 6900 / 7029\n",
      "\t\t 7000 / 7029\n",
      "\t\tConverting to numpy...\n",
      "\t\tShuffle dataset\n",
      "\t\tSplitting in train and test\n",
      "\t * dataset created;\n",
      "\t * Evaluated before train ;\n",
      "\t * Adding new classes ;\n",
      "\tCreating dataset...\n",
      "\t\t 5900 / 1165\n",
      "\t\t 6000 / 1165\n",
      "\t\t 6100 / 1165\n",
      "\t\t 6200 / 1165\n",
      "\t\t 6300 / 1165\n",
      "\t\t 6400 / 1165\n",
      "\t\t 6500 / 1165\n",
      "\t\t 6600 / 1165\n",
      "\t\t 6700 / 1165\n",
      "\t\t 6800 / 1165\n",
      "\t\t 6900 / 1165\n",
      "\t\t 7000 / 1165\n",
      "\t\tConverting to numpy...\n",
      "\t\tShuffle dataset\n",
      "\t\tSplitting in train and test\n",
      "\t * Trainning;\n",
      "1 {1: [1], 2: [2], 3: [3], 4: [4], 5: [5], 6: [6], 7: [7], 8: [8], 9: [9], 10: [10], 11: [11], 12: [12], 13: [13], 14: [14], 15: [15], 16: [16], 17: [17], 18: [18], 19: [19], 20: [20], 34: [34], 38: [38], 27: [27], 32: [32], 33: [33], 28: [28], 26: [26], 21: [21], 37: [37], 40: [40], 36: [36], 30: [30], 35: [35], 29: [29], 31: [31], 39: [39], 24: [24], 25: [25], 22: [22], 23: [23], 43: [43], 41: [41], 44: [44], 54: [54], 45: [45], 52: [52], 60: [60], 46: [46], 49: [49], 50: [50], 53: [53], 59: [59], 55: [55], 47: [47], 58: [58], 51: [51], 56: [56], 48: [48], 57: [57], 42: [42], 77: [77], 64: [64], 74: [74], 71: [71], 80: [80], 73: [73], 63: [63], 62: [62], 78: [78], 65: [65], 75: [75], 72: [72], 67: [67], 70: [70], 79: [79], 76: [76], 66: [66], 69: [69], 68: [68], 61: [61], 81: [81], 85: [85], 86: [86], 99: [99], 92: [92], 91: [91], 94: [94], 100: [100], 93: [93], 97: [97], 89: [89], 90: [90], 82: [82], 95: [95], 87: [87], 84: [84], 98: [98], 88: [88], 96: [96], 83: [83], 107: [107], 103: [103], 108: [108], 113: [113], 105: [105], 104: [104], 102: [102], 119: [119], 115: [115], 110: [110], 111: [111], 117: [117], 116: [116], 120: [120], 112: [112], 114: [114], 118: [118], 101: [101], 106: [106], 109: [109]}\n",
      "(4920, 500, 335, 3) (4920, 120)\n",
      "Train on 4428 samples, validate on 492 samples\n",
      "Epoch 1/100000\n",
      "  16/4428 [..............................] - ETA: 1:20:52WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: \n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[16,512,63,42] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node resnet50/conv3_block4_3_conv/Conv2D (defined at /home/everton/anaconda3/envs/ammd/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_distributed_function_1359033]\n\nFunction call stack:\ndistributed_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-0ffc9a7b8211>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;31m# train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\t * Trainning;'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m     \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m     \u001b[0;31m# Evaluate after train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-546338611b14>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-48-f3c84c239bad>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m         \u001b[0mY_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSeries\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mY_to_this_nivel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0;31m# Send images to branches nodes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-46-51e49072c578>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, X, Y)\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[0mcb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmonitor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'val_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpatience\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mY\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_split\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mremove_class\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx_to_remove\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ammd/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    726\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    727\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 728\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    729\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    730\u001b[0m   def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/ammd/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, **kwargs)\u001b[0m\n\u001b[1;32m    322\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m                 total_epochs=epochs)\n\u001b[0m\u001b[1;32m    325\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ammd/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    121\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    122\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    124\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ammd/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     84\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 86\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ammd/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    455\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    456\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 457\u001b[0;31m     \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    458\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    459\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_counter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcalled_without_tracing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ammd/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    519\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 520\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    521\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    522\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ammd/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1821\u001b[0m     \u001b[0;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1822\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1823\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1824\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1825\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ammd/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1141\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1142\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ammd/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1222\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[0;32m-> 1224\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[1;32m   1225\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/ammd/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 511\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    512\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/anaconda3/envs/ammd/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m     \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[0;32m~/anaconda3/envs/ammd/lib/python3.6/site-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[16,512,63,42] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node resnet50/conv3_block4_3_conv/Conv2D (defined at /home/everton/anaconda3/envs/ammd/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py:1751) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_distributed_function_1359033]\n\nFunction call stack:\ndistributed_function\n"
     ]
    }
   ],
   "source": [
    "for k in range(10):\n",
    "    print('In iteration: ', (k+1))\n",
    "    x_train, y_train, x_test, y_test = create_dataset(num_classes=(k+1)*20)\n",
    "    print('\\t * dataset created;')\n",
    "    # Evaluate before train\n",
    "    Y_hat = tree.inference(x_test)\n",
    "    print('\\t * Evaluated before train ;')\n",
    "    Y_true = y_test\n",
    "    acc = np.sum(Y_true==Y_hat)/len(Y_true)\n",
    "    accuracy_before_retrain.append(acc)\n",
    "    \n",
    "    # Add 20 new classes\n",
    "    if k != 0:\n",
    "        print('\\t * Adding new classes ;')\n",
    "        x_train_to_add, y_train_to_add, _, _ = create_dataset(num_classes=20, start_at=((k*20)+1))\n",
    "        to_insert = [x_train_to_add[y_train_to_add==idx] for idx in np.unique(y_train_to_add)]\n",
    "        labels = [idx for idx in np.unique(y_train_to_add)]\n",
    "        tree.addTasks(to_insert, labels)\n",
    "        \n",
    "    # train\n",
    "    print('\\t * Trainning;')\n",
    "    tree.train(x_train, y_train)\n",
    "    \n",
    "    # Evaluate after train\n",
    "    print('\\t * Evaluated after train ;')\n",
    "    Y_hat = tree.inference(x_test)\n",
    "    Y_true = y_test\n",
    "    acc = np.sum(Y_true==Y_hat)/len(Y_true)\n",
    "    accuracy_after_retrain.append(acc)\n",
    "    \n",
    "    x_train = None\n",
    "    y_train = None\n",
    "    x_test = None\n",
    "    y_test = None\n",
    "    Y_true = None\n",
    "    Y_hat = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.050746268656716415,\n",
       " 0.033478893740902474,\n",
       " 0.029636711281070746,\n",
       " 0.024251069900142655,\n",
       " 0.02784090909090909]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_after_retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.050746268656716415,\n",
       " 0.036390101892285295,\n",
       " 0.022944550669216062,\n",
       " 0.03138373751783167,\n",
       " 0.0375,\n",
       " 0.051683262209578]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_before_retrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
